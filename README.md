# ðŸš€ QuanTool Core: A Post-Training Quantization Toolkit

## ðŸ“Œ 1. Project Description
QuanTool is a **Python-based benchmarking framework** for evaluating the performance impact of **post-training quantization** on Hugging Face Transformer models.

It provides a clean, automated pipeline to compare a model's **original full-precision (Float32)** version against its **8-bit integer (INT8)** counterparts.

The goal is to deliver **data-driven insights** into the trade-offs between:

- âœ… Model accuracy  
- âš¡ Inference speed (latency)  
- ðŸ’¾ Memory usage  
- ðŸ“¦ Model size  

This enables developers to make **informed deployment decisions** for optimized models.

---

## âš™ï¸ 2. Installation
Ensure you have **Python 3.8+** and `pip` installed.

1. Clone the repository or download the source files.  
2. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```

---

## â–¶ï¸ 3. Running the Benchmark
The main entry point is **`run_benchmark.py`**.  
You can specify the model and **GLUE task** using command-line arguments.

### Example Commands:
```bash
# Run on DistilBERT with the default task (sst2)
python run_benchmark.py --model-name "distilbert-base-uncased"

# Run on BERT with the default task (sst2)
python run_benchmark.py --model-name "bert-base-uncased"

# Run on DistilBERT with a different task (qnli)
python run_benchmark.py --model-name "distilbert-base-uncased" --task "qnli"
```

---

## ðŸ”„ 4. What Happens When You Run It?
The script will:

1. ðŸ“¥ Download the specified model and **GLUE task dataset** (e.g., `sst2`, `qnli`).  
2. ðŸ§ª Benchmark the **original FP32 model**.  
3. âš¡ Apply and benchmark **INT8 Dynamic Quantization**.  
4. ðŸ› ï¸ Attempt **INT8 Static Quantization** (and gracefully handle failures).  
5. ðŸ“Š Print a **summary table** in the console.  
6. ðŸ’¾ Save results to:  
   - `.csv` file  
   - `.png` summary plot  

---

## ðŸ“ˆ 5. Example Output
After a successful run, you will see a table like this in your terminal.  
This example is for `distilbert-base-uncased` on the **sst2** task.  

*(Note: Numbers may vary depending on your hardware.)*

| Model        | Accuracy | Accuracy Drop | Avg Latency (ms) | Latency Speedup (x) |
|--------------|----------|---------------|------------------|----------------------|
| Float32      | 0.8933   | 0.0000        | 2753.45          | 1.00                 |
| INT8-Dynamic | 0.8853   | 0.0080        | 1305.12          | 2.11                 |

> âœ… Dynamic quantization achieves a **2.1x latency speedup** and **3.8x model size reduction** with only a **0.8% accuracy drop**.  
> 
